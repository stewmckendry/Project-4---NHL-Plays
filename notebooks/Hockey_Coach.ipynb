{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 1: Setups and Imports** <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Always run this cell first! It contains all necessary imports.\n",
    "import gc\n",
    "import sys\n",
    "import pprint\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import gym # openai gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 2: Functions** <a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1. Domain Functions** <a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2. Utility Functions** <a id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 3: Load Data** <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to nhl_play_by_play.csv\n"
     ]
    }
   ],
   "source": [
    "# Example NHL play-by-play API URL (game ID will change per game)\n",
    "game_id = \"2022030411\"  # Change this to other game IDs if needed\n",
    "url = f\"https://api-web.nhle.com/v1/gamecenter/{game_id}/play-by-play\"\n",
    "\n",
    "# Fetch data from API\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract relevant events\n",
    "    events = data.get(\"plays\", [])\n",
    "    away_Team = data.get(\"awayTeam\", [])\n",
    "    home_Team = data.get(\"homeTeam\", [])\n",
    "    extracted_data = []\n",
    "\n",
    "    for event in events:\n",
    "        event_details = event.get(\"details\", {})\n",
    "        event_data = {\n",
    "            \"eventId\": event.get(\"eventId\"),\n",
    "            \"period\": event.get(\"periodDescriptor\", {}).get(\"number\"),\n",
    "            \"time\": event.get(\"timeInPeriod\"),\n",
    "            \"away_team\": away_Team.get(\"abbrev\"),\n",
    "            \"home_team\": home_Team.get(\"abbrev\"),\n",
    "            \"home_team_side\": event.get(\"homeTeamDefendingSide\"),\n",
    "            \"eventType\": event.get(\"typeDescKey\"),\n",
    "        }\n",
    "        # Add event details to event_data\n",
    "        if event_details:\n",
    "            for key, value in event_details.items():\n",
    "                event_data[key] = value\n",
    "        \n",
    "        extracted_data.append(event_data)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(extracted_data)\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"../data/csv/nhl_play_by_play.csv\", index=False)\n",
    "    print(\"Data saved to nhl_play_by_play.csv\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve data from NHL API. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of eventType: ['period-start' 'faceoff' 'hit' 'blocked-shot' 'stoppage' 'takeaway'\n",
      " 'giveaway' 'shot-on-goal' 'missed-shot' 'penalty' 'goal'\n",
      " 'delayed-penalty' 'period-end' 'game-end']\n"
     ]
    }
   ],
   "source": [
    "print(f\"List of eventType: {df['eventType'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of eventType after filtering: ['hit' 'blocked-shot' 'takeaway' 'giveaway' 'shot-on-goal' 'missed-shot'\n",
      " 'goal']\n"
     ]
    }
   ],
   "source": [
    "# Define relevant play-driving events\n",
    "play_events = [\"shot-on-goal\", \"goal\", \"hit\", \"blocked-shot\", \"takeaway\", \"giveaway\", \"missed-shot\"]\n",
    "\n",
    "# Filter dataset to keep only these events\n",
    "df_filtered = df[df[\"eventType\"].str.lower().isin(play_events)]\n",
    "\n",
    "print(f\"List of eventType after filtering: {df_filtered['eventType'].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of columns: Index(['eventId', 'period', 'time', 'away_team', 'home_team', 'home_team_side',\n",
      "       'eventType', 'eventOwnerTeamId', 'losingPlayerId', 'winningPlayerId',\n",
      "       'xCoord', 'yCoord', 'zoneCode', 'hittingPlayerId', 'hitteePlayerId',\n",
      "       'blockingPlayerId', 'shootingPlayerId', 'reason', 'playerId',\n",
      "       'shotType', 'goalieInNetId', 'awaySOG', 'homeSOG', 'secondaryReason',\n",
      "       'typeCode', 'descKey', 'duration', 'committedByPlayerId',\n",
      "       'drawnByPlayerId', 'scoringPlayerId', 'scoringPlayerTotal',\n",
      "       'assist1PlayerId', 'assist1PlayerTotal', 'awayScore', 'homeScore',\n",
      "       'discreteClip', 'assist2PlayerId', 'assist2PlayerTotal',\n",
      "       'servedByPlayerId'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f\"List of columns: {df_filtered.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selected Key Columns**\n",
    "Since the dataset has 39 columns, I reduced it to only the necessary ones for our RL model:\n",
    "\n",
    "- Game Context: eventId, period, time, away_team, home_team, home_team_side\n",
    "- Event Type: eventType, eventOwnerTeamId\n",
    "- Location Data: xCoord, yCoord (where the event happened)\n",
    "- Shot Details (if applicable): shotType, goalieInNetId\n",
    "- Game Score: awayScore, homeScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only relevant columns\n",
    "df_filtered = df_filtered[[\n",
    "    \"eventId\", \"period\", \"time\", \"away_team\", \"home_team\", \"home_team_side\",\n",
    "    \"eventType\", \"eventOwnerTeamId\", \"xCoord\", \"yCoord\", \"zoneCode\",\n",
    "    \"shotType\", \"goalieInNetId\", \"awayScore\", \"homeScore\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of saved data:\n",
      "   eventId  period   time away_team home_team home_team_side     eventType  \\\n",
      "0        8       1  00:23       FLA       VGK          right           hit   \n",
      "1        9       1  00:27       FLA       VGK          right           hit   \n",
      "2       10       1  00:32       FLA       VGK          right           hit   \n",
      "3       53       1  00:51       FLA       VGK          right  blocked-shot   \n",
      "4       12       1  01:08       FLA       VGK          right      takeaway   \n",
      "\n",
      "   eventOwnerTeamId  xCoord  yCoord zoneCode shotType  goalieInNetId  \\\n",
      "0              13.0    89.0    31.0        O      NaN            NaN   \n",
      "1              13.0   -14.0    40.0        N      NaN            NaN   \n",
      "2              54.0   -86.0    36.0        O      NaN            NaN   \n",
      "3              54.0   -85.0   -13.0        D      NaN            NaN   \n",
      "4              54.0   -28.0   -26.0        O      NaN            NaN   \n",
      "\n",
      "   awayScore  homeScore  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "Filtered data saved as nhl_filtered_play_by_play.csv\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "df_filtered.to_csv(\"../data/csv/nhl_filtered_play_by_play.csv\", index=False)\n",
    "\n",
    "saved_data = pd.read_csv(\"../data/csv/nhl_filtered_play_by_play.csv\")\n",
    "print(f\"Sample of saved data:\\n{saved_data.head()}\")\n",
    "print(\"Filtered data saved as nhl_filtered_play_by_play.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: [ 1. 23. 89. 31.  0.]\n",
      "Random Action Taken: 2\n",
      "Next Observation: [  1.  27. -14.  40.   0.]\n",
      "Reward Received: 1\n",
      "Done: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liammckendry/hockey_ai/lib/python3.13/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned play-by-play data\n",
    "df_filtered = pd.read_csv(\"../data/csv/nhl_filtered_play_by_play.csv\")\n",
    "\n",
    "# Define the Hockey RL Environment (an instance of the OpenAI Gym environment for Reinforcement Learning)\n",
    "class HockeyPlayRL(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        super(HockeyPlayRL, self).__init__()\n",
    "\n",
    "        # Save data\n",
    "        self.df = df.reset_index(drop=True) # Reset index to avoid issues with step function\n",
    "\n",
    "        # Define action space (4 possible actions: pass, shoot, carry, dump)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Define observation space (state variables: period, time, x, y, score diff)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([1, 0, -100, -50, -10]),   # Min values for period, time, xCoord, yCoord, scoreDiff\n",
    "            high=np.array([3, 1200, 100, 50, 10]), # Max values for period, time, xCoord, yCoord, scoreDiff\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Initialize game state\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment at the beginning of an episode.\"\"\"\n",
    "        self.current_step = 0\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        \"\"\"Get current game state as RL observation.\"\"\"\n",
    "        row = self.df.iloc[self.current_step]\n",
    "\n",
    "        # Convert game state into numerical features\n",
    "        period = row[\"period\"]\n",
    "        time = int(row[\"time\"].split(\":\")[0]) * 60 + int(row[\"time\"].split(\":\")[1])  # Convert time to seconds\n",
    "        xCoord = row[\"xCoord\"] if not np.isnan(row[\"xCoord\"]) else 0\n",
    "        yCoord = row[\"yCoord\"] if not np.isnan(row[\"yCoord\"]) else 0\n",
    "        scoreDiff = (row[\"awayScore\"] - row[\"homeScore\"]) if not np.isnan(row[\"awayScore\"]) else 0\n",
    "\n",
    "        return np.array([period, time, xCoord, yCoord, scoreDiff], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply an action and transition to the next state.\"\"\"\n",
    "        row = self.df.iloc[self.current_step]\n",
    "\n",
    "        # Assign rewards based on event type\n",
    "        reward = 0\n",
    "        if row[\"eventType\"] == \"hit\":\n",
    "            reward = 1\n",
    "        elif row[\"eventType\"] == \"blocked-shot\":\n",
    "            reward = 5\n",
    "        elif row[\"eventType\"] == \"takeaway\":\n",
    "            reward = 3\n",
    "        elif row[\"eventType\"] == \"giveaway\":\n",
    "            reward = -5\n",
    "        elif row[\"eventType\"] == \"shot-on-goal\":\n",
    "            reward = 5\n",
    "        elif row[\"eventType\"] == \"missed-shot\":\n",
    "            reward = -2\n",
    "        elif row[\"eventType\"] == \"goal\":\n",
    "            reward = 10\n",
    "\n",
    "        # Move to the next step\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Check if game is over (end of dataset)\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "\n",
    "        return self._next_observation(), reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"Optional visualization (for debugging).\"\"\"\n",
    "        pass\n",
    "\n",
    "# Initialize the environment\n",
    "hockey_env = HockeyPlayRL(df_filtered)\n",
    "\n",
    "# Test environment reset and step\n",
    "obs = hockey_env.reset()\n",
    "action_sample = hockey_env.action_space.sample()\n",
    "next_obs, reward, done, _ = hockey_env.step(action_sample)\n",
    "\n",
    "print(\"Initial Observation:\", obs)\n",
    "print(\"Random Action Taken:\", action_sample)\n",
    "print(\"Next Observation:\", next_obs)\n",
    "print(\"Reward Received:\", reward)\n",
    "print(\"Done:\", done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liammckendry/hockey_ai/lib/python3.13/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize Hockey RL Environment\n",
    "env = HockeyPlayRL(df_filtered)\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Deep Q-Network (DQN) model for RL of Hockey Play-by-Play\n",
    "# Why? → This neural network predicts Q-values for all actions based on the current hockey play.\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize networks\n",
    "# Why? → We use two networks:\n",
    "# Online Network (learning agent)\n",
    "# Target Network (stable reference)\n",
    "# input_dim = 5 (period, time, xCoord, yCoord, scoreDiff)\n",
    "# output_dim = 7 (number of actions - hit, blocked_shot, takeaway, giveaway, shot_on_goal, missed_shot, goal)\n",
    "online_network = DQN(input_dim=5, output_dim=7).to(device)\n",
    "target_network = DQN(input_dim=5, output_dim=7).to(device)\n",
    "target_network.load_state_dict(online_network.state_dict())  # Copy weights\n",
    "target_network.eval()  # Target network is frozen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Experience Replay Memory\n",
    "# Why? → Stores past experiences so the AI can learn from past plays instead of just the most recent ones.\n",
    "# Will be used to train the model - stores Q-values and rewards for each action taken.\n",
    "# Experience = (state, action, reward, next_state, done)\n",
    "# Memory = list of experiences\n",
    "memory = []\n",
    "max_memory_size = 10000  # Store last 10,000 experiences\n",
    "\n",
    "def store_experience(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "    if len(memory) > max_memory_size:\n",
    "        memory.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Epsilon-Greedy Strategy\n",
    "# Why? → Starts with exploration (ε = 1.0 random actions) and gradually learns to exploit the best moves.\n",
    "epsilon = 1.0  # Start fully random\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995  # Reduce randomness over time\n",
    "\n",
    "def select_action(state):\n",
    "    if np.random.rand() < epsilon:  # Explore (random action)\n",
    "        return np.random.randint(0, 7)  # Updated to match output_dim (7 actions)\n",
    "    else:  # Exploit (use learned Q-values)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(online_network(state_tensor)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function & Optimizer\n",
    "# Why? → We use Mean Squared Error Loss and Adam Optimizer to train the model.\n",
    "# Why? → This updates the online network so it learns which hockey plays lead to the best outcomes.\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.Adam(online_network.parameters(), lr=0.001)  # Adam Optimizer\n",
    "batch_size = 32\n",
    "gamma = 0.99  # Discount factor for future rewards\n",
    "\n",
    "def train_network():\n",
    "    if len(memory) < batch_size:\n",
    "        return  # Don't train until enough experiences are collected\n",
    "\n",
    "    batch = random.sample(memory, batch_size)  # Sample batch from memory\n",
    "\n",
    "    for state, action, reward, next_state, done in batch:\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "        action = torch.tensor(action).to(device)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Compute Q-value for current state-action pair\n",
    "        q_values = online_network(state)\n",
    "        q_value = q_values[action]\n",
    "\n",
    "        # Compute target Q-value\n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_network(next_state)\n",
    "            target_q_value = reward + (gamma * torch.max(next_q_values)) * (1 - done)\n",
    "\n",
    "        # Compute loss & update weights\n",
    "        loss = loss_fn(q_value, target_q_value)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Target Network Update\n",
    "# Why? → The target network is updated every few episodes to keep learning stable.\n",
    "def update_target_network():\n",
    "    target_network.load_state_dict(online_network.state_dict())  # Copy weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epsilon_decay\n",
    "# Why? → Reduces randomness over time as the AI learns to exploit the best moves.\n",
    "def update_epsilon():\n",
    "    global epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)  # Reduce randomness over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 completed. Epsilon: 0.99\n",
      "Episode 100 completed. Epsilon: 0.60\n",
      "Episode 200 completed. Epsilon: 0.37\n",
      "Episode 300 completed. Epsilon: 0.22\n",
      "Episode 400 completed. Epsilon: 0.13\n",
      "Episode 500 completed. Epsilon: 0.10\n",
      "Episode 600 completed. Epsilon: 0.10\n",
      "Episode 700 completed. Epsilon: 0.10\n",
      "Episode 800 completed. Epsilon: 0.10\n",
      "Episode 900 completed. Epsilon: 0.10\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train the Deep Q-Network (DQN) for Hockey Play-by-Play\n",
    "# This runs thousands of simulated hockey games until the AI masters play selection. 🏆\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()  # Reset game\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = select_action(state)  # Step 3: Choose action\n",
    "        next_state, reward, done, _ = env.step(action)  # Step 4: Take action\n",
    "        store_experience(state, action, reward, next_state, done)  # Step 4: Store experience\n",
    "        train_network()  # Step 5: Train neural network\n",
    "        state = next_state  # Move to next state\n",
    "\n",
    "    update_target_network()  # Step 6: Update target network every episode\n",
    "    update_epsilon()  # Step 7: Reduce exploration (ε)\n",
    "\n",
    "    # Print progress every 100 episodes\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} completed. Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model - online network\n",
    "with open(\"../src/models/dqn_model-online_network.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(online_network, model_file)\n",
    "\n",
    "# Save the trained model - target network\n",
    "with open(\"../src/models/dqn_model-target_network.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(target_network, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define action mapping\n",
    "action_mapping = {\n",
    "    0: \"Hit\",\n",
    "    1: \"Blocked Shot\",\n",
    "    2: \"Takeaway\",\n",
    "    3: \"Giveaway\",\n",
    "    4: \"Shot on Goal\",\n",
    "    5: \"Missed Shot\",\n",
    "    6: \"Goal\"\n",
    "}\n",
    "\n",
    "# Open the log file in append mode\n",
    "log_file = \"../outputs/hockey_action_log.csv\"\n",
    "\n",
    "# Write headers if file is empty\n",
    "try:\n",
    "    with open(log_file, \"x\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Period\", \"Time\", \"X Coord\", \"Y Coord\", \"Score Diff\", \"Action\", \"Action Name\"])\n",
    "except FileExistsError:\n",
    "    pass  # File already exists, so no need to create it again\n",
    "\n",
    "def predict_best_action(state):\n",
    "    if np.random.rand() < epsilon:  # Explore (random action)\n",
    "        action = np.random.randint(0, 7)\n",
    "    else:  # Exploit (use learned Q-values)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            action = torch.argmax(online_network(state_tensor)).item()\n",
    "\n",
    "    action_name = action_mapping[action]\n",
    "\n",
    "    # Print action for debugging\n",
    "    print(f\"Selected Action: {action} -> {action_name}\")\n",
    "\n",
    "    # Log state and action\n",
    "    with open(log_file, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(state + [action, action_name])\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Action: 2 -> Takeaway\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model\n",
    "new_state = [3, 400, 45, 30, 0]  # Example game state (period, time, xCoord, yCoord, scoreDiff)\n",
    "best_action = predict_best_action(new_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 4: Pre-Process Data** <a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 5: Model** <a id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1. Model Definition** <a id=\"5.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **5.2. Model Training** <a id=\"5.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **5.3. Model Evaluation** <a id=\"5.3\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hockey_ai)",
   "language": "python",
   "name": "hockey_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
